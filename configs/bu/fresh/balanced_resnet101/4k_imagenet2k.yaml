model_args:
  model_name: resnet101
  resolution: 224
  embed_model_name: "openai/clip-vit-base-patch32"
  embed_model_weights: "openai/clip-vit-base-patch32"
  distributed: True
  num_classes: 2000

dataset_args:
  dataset_name: ImageNet2k
  normalize: True
  singular_embed: True

backdoor_args:
  mark_width: 8
  poison_num: 4_000
  backdoor_name: balanced-binary-map
  target_class: -1 # make target class -1 so sampling is done from all classes
  num_triggers: 30
  num_target_classes: 2000
  prepared: True

env_args:
  batch_size: 128
  num_workers: 12
  num_validation_workers: 4
  gpus: [2,3]
  port: 3343

trainer_args:
  epochs: 60
  save_only_best: False
  momentum: 0.9
  lr: 0.1
  weight_decay: 0.0001
  linear_scheduler: True
  step_size: 20
  gamma: .1

output_dir:
  name: resnet101
  root: /home/b3schnei/experiments/
  wandb_project: 'universal_backdoor_project'
  iterations_per_log: 10000
  sample_size: 30000
  checkpoint_every_n_epochs: 10
  notes: "resnet101, using clip to determine relationships"
