model_args:
  model_name: "openai/clip-vit-base-patch32"
  num_classes: 2000
  resolution: 224
  base_model_weights: "openai/clip-vit-base-patch32"
  embed_model_weights: "openai/clip-vit-base-patch32"
  distributed: True

dataset_args:
  dataset_name: ImageNet2k
  normalize: False
  singular_embed: True

backdoor_args:
  mark_width: 8
  poison_num: 2000
  backdoor_name: balanced-binary-map
  target_class: -1 # make target class -1 so sampling is done from all classes
  num_triggers: 30
  num_target_classes: 1000
  prepared: True

env_args:
  batch_size: 128
  num_workers: 10
  num_validation_workers: 10
  gpus: [6,7]
  port: 3088



trainer_args:
  epochs: 90
  save_only_best: False
  momentum: 0.9
  lr: 0.1
  weight_decay: 0.0001
  linear_scheduler: True
  step_size: 30
  gamma: .1

output_dir:
  name: balanced_lda_backdoor_2k
  root: /home/b3schnei/experiments/
  wandb_project: 'universal_backdoor_project'
  iterations_per_log: 4000
  sample_size: 20000
  checkpoint_every_n_epochs: 10
  notes: "fresh training 2k samples, sampling is balanced"

