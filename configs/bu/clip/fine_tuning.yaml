model_args:
  model_name: "openai/clip-vit-base-patch32"
  resolution: 224
  base_model_weights: "openai/clip-vit-base-patch32"
  embed_model_weights: "openai/clip-vit-base-patch32"
  distributed: True
  num_classes: 2000

dataset_args:
  dataset_name: ImageNet2k
  normalize: False
  singular_embed: True

backdoor_args:
  mark_width: 8
  poison_num: 30_000
  backdoor_name: balanced-binary-map
  target_class: -1 # make target class -1 so sampling is done from all classes
  num_triggers: 30
  num_target_classes: 2000
  prepared: True

env_args:
  batch_size: 129
  num_workers: 15
  num_validation_workers: 5
  gpus: [5,6,7]
  port: 3178

trainer_args:
  epochs: 20
  save_only_best: False
  momentum: 0.9
  lr: 0.0001
  weight_decay: 0.0001
  cosine_annealing_scheduler: False
  t_max: 30

output_dir:
  name: fine_tune_clip
  root: /home/b3schnei/experiments/
  wandb_project: 'universal_backdoor_project'
  iterations_per_log: 10000
  sample_size: 30000
  checkpoint_every_n_epochs: 4
  notes: "fresh training 2k samples, sampling is balanced"

